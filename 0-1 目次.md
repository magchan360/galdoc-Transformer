### **はじめに：なぜ、この一編の論文が世界を変えたのか？**
*   2017年、AIの世界で静かな革命が起きた
*   本書が約束すること：数式が苦手でも「気持ち」がわかる
*   さあ、現代AIの源流へ旅に出よう

---

### **第1部：Transformer前夜 - 我々は「順番」に縛られていた**

*   **第1章：言葉を「ベクトル」で表現するということ**
    *   1.1 Word2Vecの衝撃：単語に「住所」を与える
    *   1.2 「王様 - 男性 + 女性 = 女王様」の魔法
*   **第2章：順番が命！RNNの時代**
    *   2.1 時系列を扱うモデルの誕生：Recurrent Neural Network
    *   2.2 過去の記憶を保持する工夫：LSTMとGRU
    *   2.3 RNNの限界：遠い過去は忘れ去られ、計算は遅かった
*   **第3章：革命の序曲 - Seq2Seqと「最初のAttention」**
    *   3.1 翻訳モデルのブレークスルー：Seq2Seq
    *   3.2 「どこに注目すべきか」を教えるAttention機構の登場
    *   3.3 それでも残ったRNNの呪縛

---

### **第2部：徹底解剖 - Transformerアーキテクチャのすべて**

*   **第4章：論文の全体像 - Encoder-Decoderモデル**
    *   4.1 Transformerがやろうとしていること：入力文から出力文への「変換」
    *   4.2 6つの箱の積み重ね：EncoderとDecoderの役割分担
*   **第5章：心臓部 "Attention" のメカニズム**
    *   5.1 すべてはここから：Query, Key, Valueとは何か？
    *   5.2 図で理解する「Scaled Dot-Product Attention」
    *   5.3 なぜスケール（√dk）で割るのか？Softmaxの罠
    *   5.4 Multi-Head Attention：なぜ複数の「視点」が必要なのか？
*   **第6章：文脈を読み解く機械 - Encoderの深層**
    *   6.1 Self-Attention：文章が「自分自身」を理解する仕組み
    *   6.2 Position-wise Feed-Forward Networks：Attentionの出力を「こねる」層
    *   6.3 天才的なお助け機能：Add & Norm (残差接続と層正規化)
*   **第7章：文章を生成する機械 - Decoderの深層**
    *   7.1 未来は見るな！Masked Self-Attentionの巧妙な仕掛け
    *   7.2 Encoderからの情報をカンニングする「Encoder-Decoder Attention」
*   **第8章：最後の仕上げと、忘れられた英雄**
    *   8.1 最終出力を得るためのLinear層とSoftmax層
    *   8.2 「順番」の情報を与える魔法：Positional Encodingの数学的意味

---

### **第3部：なぜ世界はTransformerを選んだのか？**

*   **第9章：性能と効率の二重革命**
    *   9.1 長期依存関係の克服：遠くの単語も関係がわかる！
    *   9.2 GPUとの奇跡的な相性：並列計算がもたらした学習速度の爆発
*   **第10章：Transformerから生まれた子孫たち**
    *   10.1 BERTの衝撃：「文脈を読む」ことの真の意味
    *   10.2 GPTの躍進：流暢な文章を生成するモンスター
    *   10.3 自然言語処理から画像、音声へ：Transformerの驚くべき汎用性

---

### **おわりに：Attentionの先に見える未来**
*   Transformerはゴールではなく、スタート地点
*   LLMのこれからと、私たちが向き合うべき課題

---
