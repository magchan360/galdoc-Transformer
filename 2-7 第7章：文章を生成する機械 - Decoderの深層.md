## 第7章：文章を生成する機械 - Decoderの深層

  Encoderが文章を「理解する」プロだったよね。じゃあ、Decoderは？
  こっちは、その理解を元に、新しい文章を「生み出す」プロ集団！
  翻訳したり、質問に答えたり、文章を要約したり…マジでクリエイティブな仕事を担当してるんだ。

  Decoderのフロアには、Encoderにはなかった、超巧妙な秘密兵器が2つも隠されてる。
  その秘密、アゲハと一緒に暴いちゃお！

### 7.1 未来は見るな！Masked Self-Attentionの巧妙な仕掛け

  Decoderも、文章を作る上で「この単語とこの単語、関係深いよね」ってチェックするために、Self-Attentionを持ってる。
  でも、DecoderのSelf-Attentionには、一つだけ絶対に破れない鉄の掟があるんだ。

  それは、「絶対に未来の単語をカンニングしちゃダメ！」ってこと！

  【例】AIが「今日の天気は晴れです」という文章を作ろうとしてる場合

  AIは、単語を一個ずつ順番に作っていくよね。

   1. まず、最初の単語「今日」を生成する。
   2. 次に、「の」を生成しようと考える。この時、AIが見ていいのは、すでに出力した「今日」だけ。未来にある「天気」とか「晴れ」とかが見えちゃったら、それはもうカンニングじゃん？
   3. さらに、「天気」を生成する時は、「今日 の」までしか見ちゃダメ。

  答えを見ながらテスト受けてるのと同じで、そんなことしてたらAIが全然賢くならないんだよね。

  そこで登場するのが、秘密兵器その①「Masked Self-Attention」！
  これは、Self-Attentionに特殊な「未来が見えなくなる目隠し（マスク）」を付けたバージョンなんだ。

  AIが「天気」を作ろうとしてる時、このマスクが「は」「晴れ」「です」っていう未来の単語を、強制的に見えなくしてくれる。
  この巧妙な「未来隠し」があるから、AIはちゃんと自力で、「『今日の』次に来る単語は、何が一番自然かな？」って予測するトレーニングができるってワケ！

### 7.2 Encoderからの情報をカンニングする「Encoder-Decoder Attention」

  未来は見えなくなった。でも、それだけじゃ、AIは何をヒントに文章を作ればいいか分かんないよね。
  例えば、質問が「日本の首都は？」だったのに、AIが「今日の天気は晴れです」って答えちゃったら、マジでヤバいじゃん。

  そこで登場するのが、秘密兵-器その②「Encoder-Decoder Attention」！
  これこそが、入力された文章（質問文）の情報を、正々堂々とカンニングするための、超重要な仕組みなんだ！

  Decoderの1フロアには、実はAttentionが2種類あるってこと。

   4. Masked Self-Attention:
      自分が今作ってる文章だけを見て、「日本語として変じゃないかな？」ってチェックする。（文章のセルフチェック）
   5. Encoder-Decoder Attention:
      Encoderが完璧に理解した「質問文」の情報をガッツリ見て、「何について答えるべきか」をチェックする。（質問文のカンニング）

---
#### Encoder-Decoder Attention：例

  Encoder-Decoder Attentionは、一言でいうと、デコーダーがエンコーダーの分析結果を、超絶カンニングする時間のこと！
  デコーダーが、次にどの単語を作ればいいか迷ったとき、「えーっと、元の文章のどこを見ればヒントがあるかな？」って、カンニングペーパー（エンコーダーの分析結果）をガン見する
  イメージだね。

  例：翻訳タスク「その美しい少女がリンゴを食べた」

  この、日本語と英語で語順がぜんぜん違う文章を、AIが翻訳するシーンを想像してみて！

  入力文（日本語）： 「その 美しい 少女 が リンゴ を 食べた」
  出力文（英語）： 「The beautiful girl ate an apple」

  まず、エンコーダーが日本語の文章を読んで、単語一つ一つの意味や役割を完璧に分析してくれる。この分析結果が、デコーダーにとっての最強のカンニングペーパーになるワケ。

  次に、デコーダーが英語の文章を、一個ずつ作り始めるよ。

   1. 「The」を生成するとき：
       * デコーダーの気持ち（Query）： 「文の始まりは、どの単語から作るのが自然かな？」
       * カンニングタイム！ デコーダーは、入力文全体に注目用のレーザーポインターを当てる。すると、「その」っていう単語がピカーン！と強く光るんだ。
       * 結果： 「なるほど、『その』に対応する『The』から始めればいいのね！」

   2. 「beautiful」を生成するとき：
       * デコーダーの気持ち（Query）： 「次、どんな単語が来るべき？」
       * カンニングタイム！ 今度のレーザーポインターは、「美しい」をピカーン！と照らす。
       * 結果： 「OK、『美しい』を訳せばいいんだな！」

   3. 「girl」を生成するとき：
       * デコーダーの気持ち（Query）： 「で、美しいのは誰なの？」
       * カンニングタイム！ レーザーポインターは「少女」をロックオン！
       * 結果： 「はいはい、『少女』ね！ 了解！」

   4. 「ate」を生成するとき：
       * デコーダーの気持ち（Query）： 「その少女が、何をしたの？」
       * ここがマジでヤバい！ デコーダーのレーザーポインターは、文の途中を飛び越えて、日本語の文末にある「食べた」を、一直線にピカーン！と照らすんだ！
       * 結果： 「なるほど、アクションは『食べた』ことね！」

   5. 「an apple」を生成するとき：
       * デコーダーの気持ち（Query）： 「何を食べたわけ？」
       * カンニングタイム！ 最後のレーザーポインターは、「リンゴを」を正確に捉える。
       * 結果： 「OKOK、『リンゴ』ね！ これで全部つながった！」



  RNNみたいに、律儀に情報の伝言ゲームをしてたら、文末にある「食べた」っていう大事な情報を、文の真ん中に持ってくるのはマジで大変だった。

  でも、Encoder-Decoder Attentionなら、距離なんて関係ない！
  デコーダーが「今、この情報が欲しい！」って思ったら、入力文のどこにその情報があっても、一瞬でスポットライトを当てて、その情報をカンニングできるんだ。

  この「必要な情報を、必要な時に、どこからでも取ってこれる」っていう神スキルこそが、Transformerが語順の違う言語でも、超スムーズに翻訳できる秘密なんだよね！

  ---

  どう？ Decoderの賢さ、マジでヤバくない？
  「未来は見ずに自分の文章を整えつつ（Masked Self-Attention）、質問文はガッツリカンニングして（Encoder-Decoder Attention）、正しい単語を一個ずつ生み出していく」
  これが、Decoderが賢く文章を生成できる、秘密のレシピなんだよね！

---
### カンニングするなら Masked Self Attention の意味なくね？  

 オタ君、マジで天才的な質問！ それ、この論文を理解する上で、みんなが一回は絶対につまずくポイントだから！
  「カンニングできるなら、自分の文章チェックとかいらなくね？」って思うの、ちょー分かる！

  でもね、それがマジで意味あるんだな、これが！
  2つは、やってる仕事がぜんぜん違うの！

  超分かりやすい例えで言うと、デコーダーは「優秀な同時通訳さん」みたいな感じ！
  この通訳さんには、絶対に欠かせない2つの能力があるんだ。

  ---

  能力①：お客さんの話（入力文）をちゃんと聞くこと

  これが「Encoder-Decoder Attention」（カンニングタイム）の仕事！

  通訳さんは、まず、外国人の話してること（＝入力文）を、めちゃくちゃ集中して聞かなきゃいけないよね。
  お客さんが「apple」って言ったら、その「apple」っていう情報を絶対に聞き逃しちゃダメ。

  もし、この能力がなかったら、
  お客さん：「I ate an apple yesterday.」
  通訳さん：「私は昨日、オレンジを食べました。」
  みたいに、話の内容がデタラメになっちゃう！

  つまり、Encoder-Decoder Attentionは、「これから作る文章が、元の文章（入力文）の内容からズレないようにする」ための、超大事なカンニングなんだ！

  能力②：自分が今から話す日本語が、自然かどうかを確認すること

  こっちが「Masked Self-Attention」（セルフチェックタイム）の仕事！

  通訳さんは、お客さんの言った「apple」を「リンゴ」って訳すだけじゃダメだよね。
  自然で、文法的に正しい日本語の文章を、一から組み立てなきゃいけない。

  例えば、通訳さんの頭の中で、
  「『私』『は』『昨日』…」って文章を作ってるとき、
  「『昨日』の次には、『リンゴ』が来るのが自然かな？ それとも『食べた』が来るべきかな？」
  って、自分が今作ってる文章の中だけで、単語のつながりをチェックしてるんだ。

  もし、この能力がなかったら、
  お客さん：「I ate an apple yesterday.」
  通訳さん：「私 昨日 リンゴ 食べた。」
  みたいに、単語は合ってるけど、日本語としてめちゃくちゃになっちゃう！

  だから、Masked Self-Attentionは、「これから作る文章が、文法的に美しくて、自然な流になるようにする」ための、超大事なセルフチェックなんだ！

  ---

  まとめ：なぜ両方必要なのか？

  もうわかったかな？

   * もしカンニング（Encoder-Decoder Attention）しかなかったら…
      AIは「リンゴ」「少女」「食べた」みたいなキーワードは拾えるけど、それを自然な文章に組み立てられない。「食べた リンゴ  少女」みたいな、意味不明な単語の羅列になっちゃう。

   * もしセルフチェック（Masked Self-Attention）しかなかったら…
      AIは「犬が公園を散歩しています」みたいな、文法的には完璧な文章は作れる。でも、元の質問が「昨日の夕食は何？」だったとしても、完全に無視した文章を作っちゃう。

  だから、この2つは、どっちが偉いとかじゃなくて、最強のタッグなんだ！

  カンニング（Encoder-Decoder Attention）で「何を言うべきか」を理解して、
  セルフチェック（Masked Self-Attention）で「どう自然に言うか」を完成させる。

  この両方があって初めて、人間みたいな自然な文章が生まれるってワケ！
  すごくない！？