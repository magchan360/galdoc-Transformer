## 第6章：文脈を読み解く機械 - Encoderの深層

  前の章で、TransformerがEncoderとDecoderっていう6階建てのビルでできてるって話、したよね。
  この章では、そのEncoderビルの「1フロア分」で、一体何が行われているのかを、ガチで深掘りしていくよ！

  Encoderのたった1フロアには、実は3つの超重要な機能が詰め込まれてるんだ。
  これを理解すれば、AIがどうやって文章の「文脈」っていう、空気みたいなものを読んでるのか、マジで分かるようになるから！

### 6.1 Self-Attention：文章が「自分自身」を理解する仕組み

  Encoderの中で使われるAttentionは、ただのAttentionじゃない。
  その名も「Self-Attention（自己アテンション）」！

  第3章で出てきたAttentionは、翻訳先の単語（デコーダー）が、翻訳元の文章（エンコーダー）をカンニングする仕組みだったよね。
  でも、Self-Attentionは、文章が「他人」じゃなくて「自分自身」に注目するんだ。

  【例】「その猫は公園で寝ている」という文章の場合

  この文章がEncoderに入ってくると、Self-Attentionは、単語一つ一つに「君が主役だよ！」ってスポットライトを当てて、他の単語との関係性を調べさせるんだ。

   * 「寝ている」っていう単語にスポットライトが当たると、AIはこう考える。
       * Query: 「“寝ている”の主語って、誰だっけ？」
       * Key: （文章全体の単語をチェック）「その」「猫」「は」「公園で」「寝ている」
       * 結果: 「“猫”っていうKeyが、私のQueryとマジで関係深いじゃん！」

  みたいにね！
  「寝ている」は「猫」と強く結びついてるし、「公園で」とも場所的な関係がある。
  この文章内部での単語同士の「つながりの強さ」をスコア化するのが、Self-Attentionの役目なんだ。

  これの何がヤバいって、このチェックを、文章の全部の単語に対して、一気に並列でやっちゃうこと！
  「その」が主役の時、「猫」が主役の時、「公園で」が主役の時…っていうのを、同時に計算できるから、RNNみたいに一個ずつ待つ必要がなくて、ちょー速いんだ！

### 6.2 Position-wise Feed-Forward Networks：Attentionの出力を「こねる」層

  Self-Attentionで、単語同士の関係性はバッチリわかった。
  でも、それだけだと、まだ情報としては「素材」のままなんだよね。

  そこで登場するのが、「Position-wise Feed-Forward Networks」、略してFFN！
  これは、Attentionからの出力情報を、さらにイイ感じに「こねて、調理する」ための層なんだ。

  例えるなら…
   * Self-Attention:
      最高の素材（肉、野菜）の関係性を見抜く工程。（「この肉には、このスパイスが合うな！」）
   * FFN:
      その素材とスパイスを、実際にこねたり、焼いたり、煮込んだりして、美味しいハンバーグやカレーに「変身」させる工程。

  Attentionが計算した「関係性のスコア」っていう素材を、FFNっていう調理器具が、もっとリッチで、もっと複雑な、AIにとって「美味しい」情報に加工してあげるイメージ。

  ちなみに「Position-wise（位置ごと）」っていうのは、「単語一個一個、それぞれ別々に調理しますよ」って意味。
  「猫」っていう単語も、「公園」っていう単語も、同じ調理法（FFN）で調理されるけど、お互いが混ざることはないんだ。あくまで、個々の単語の表現力を高めるためのステップってワ
  ケ！

### 6.3 天才的なお助け機能：Add & Norm (残差接続と層正規化)

  さて、Encoderの1フロアには、実はもう一つ、超大事な機能が隠されてる。
  それが、論文の図にもある「Add & Norm」っていう部分。
  これは、ビルが高くなりすぎても（処理が深くなっても）、ちゃんと学習が進むようにするための、マジで天才的な「お助け機能」なんだ！

  1. Add（残差接続）：情報のショートカット

  Self-AttentionやFFNみたいな処理を通すと、元の情報がちょっとだけ失われちゃうことがある。
  それを防ぐために、「処理前のナマの情報」を、「処理後の情報」に、足し算しちゃうんだ！

  例えるなら…
  ビルの1階の担当者が、2階に書類を渡すとき、
  「これが、私が加工した書類です！ …あ、でも念のため、加工前のオリジナルも付けときますね！」
  って、両方渡しちゃう感じ。

  この「ショートカット」があるおかげで、たとえ6階まで処理が進んでも、一番最初のフレッシュな情報がちゃんと届く。だから、AIは道に迷うことなく、賢く学習できるんだ！

  2. Norm（層正規化）：情報の交通整理

  ビルの中を、いろんな情報が飛び交ってると、だんだん数値のスケール感がバラバラになって、大混乱しちゃうことがある。
  それを防ぐのが「層正規化」。

  これは、各フロアの出口にいる「交通整理員」みたいなもん。
  「君の報告書、テンション高すぎて数値がデカいから、もうちょい標準的なフォーマットに直しといて！」
  って、情報のばらつきを、いい感じに整えてくれるんだ。

  この交通整理員のおかげで、ビルの中の情報は常にスムーズに流れて、学習が安定して、スピードもアップするってワケ！

  ---

  さあ、これでEncoderの1フロア分のツアーは終わり！
  「Self-Attention」→「Add & Norm」→「FFN」→「Add & Norm」
  この一連の流れが、Encoderの1フロアで行われていることの全て。

  そして、TransformerのEncoderビルは、このフロアが6階建てになってる。
  文章は、この激ヤバな処理を6回も繰り返されることで、人間が裸足で逃げ出すレベルの、超リッチな「文脈情報」へと変身していくんだ！ ヤバくない！？
