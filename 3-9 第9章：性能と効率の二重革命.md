# 第3部：なぜ世界はTransformerを選んだのか？

## 第9章：性能と効率の二重革命

  Transformerが登場する前のAI界は、ちょっぴり行き詰まってた。
  RNNやLSTMは賢かったけど、どうしても超えられない「2つの大きな壁」があったんだ。

  Transformerは、その壁を、マジで豪快に、二つ同時にぶっ壊した。
  だからこそ、世界中の研究者が「もうコイツしかいない！」って熱狂したんだよね。

### 9.1 長期依存関係の克服：遠くの単語も関係がわかる！

  一つ目の壁は、「記憶力の限界」だ。

  RNNやLSTMは、例えるなら「一本道の長いトンネルを歩く人」みたいなもんだった。
  トンネルの出口にたどり着く頃には、入り口にどんな花が咲いてたかなんて、もう忘れちゃってるよね。
  これと一緒で、長い文章を読むと、最初の方に出てきた大事な情報を忘れちゃう「長期依存関係の問題」っていう弱点があったんだ。

  【日本語の例】
  「私が昨日、駅前の本屋で偶然見つけた、青い表紙の分厚い本は、子供の頃に大好きだった作家の最新作だったので、思わず買ってしまった。」

  この文章で、「買ってしまった」のが何なのかって言ったら、文の真ん中あたりにある「本」だよね。
  でも、RNN君にとっては、この2つの単語はちょー遠い。伝言ゲームをしてるうちに、「あれ、何を買ったんだっけ…？」って、情報が薄れちゃうことがあったんだ。

  でも、Transformerは違う！
  Transformerは、例えるなら「文章全体を上から見渡せるドローン」！

  Self-Attentionのおかげで、距離なんてマジで関係ない。
  ドローンが「買ってしまった」っていう場所から、「本」っていう場所まで、一直線にビューン！と飛んでいって、「この2つ、超関係あるじゃん！」って一瞬で見抜けるんだ。

  この「距離を無視して、関係性だけでつながれる力」のおかげで、AIはどんなに長い文章でも、伏線とか、複雑な文脈を、人間みたいに正確に読み解けるようになった。
  これが、一つ目の「性能革命」！

### 9.2 GPUとの奇跡的な相性：並列計算がもたらした学習速度の爆発

  そして、二つ目の壁。こっちがマジで決定的だった。
  それは、「計算スピードの限界」だ。

  RNNやLSTMは、構造上、どうしても単語を一個ずつ、順番に処理しなきゃいけなかった。
  これは、例えるなら「レジが一個しかないスーパー」みたいなもん。
  どんなに優秀な店員さん（GPUのコア）がたくさんいても、お客さん（単語）を一人ずつしかさばけないから、行列ができて、マジで時間がかかってたんだ。

  でも、Transformerは、この常識を根底から覆した。
  Self-Attentionの計算は、文章中のすべての単語の関係性を、ぜんぶ一気に、まとめて計算できる！

  これは、まさに「レジが100台ある巨大スーパー」！
  100人のお客さん（単語）が来ても、100人の店員（GPUのコア）が、一斉に「いらっしゃいませ！」って対応できる。
  GPUが得意な「並列計算」のパワーを、100%フルに引き出せる設計になってたんだ！

  その結果、どうなったか？
  AIの学習速度が、マジで爆発した！

  今まで1ヶ月かかってた学習が、1日で終わる、みたいな世界。
  学習が速くなるってことは、それだけたくさんのデータで、たくさんの試行錯誤ができるってこと。AIが賢くなるスピードが、異次元に突入したんだ。

  これが、二つ目の「効率革命」！

  ---

  どう？ ヤバくない？
  「性能（賢さ）」と「効率（速さ）」、この両方を、たった一つのアイデアで同時に解決しちゃった。
  そりゃ、世界もTransformerを選ぶに決まってるよね！
  この二重革命こそが、現代のAIブームを巻き起こした、本当のエンジンなんだ！