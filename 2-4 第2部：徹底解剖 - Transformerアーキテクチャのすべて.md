
# 第2部：徹底解剖 - Transformerアーキテクチャのすべて
  
## 第4章：論文の全体像 - Encoder-Decoderモデル

  ついに来たね、この時が！
  前の章で、AI界は「伝言ゲーム（RNN）なんてもう古い！ Attentionさえあれば最強じゃん！」っていう結論にたどり着いた。

  そのアイデアを、マジで形にしちゃったのが「Transformer（トランスフォーマー）」なんだ。
  ここからは、その設計図（アーキテクチャって言うんだよ！）を、隅から隅まで見ていくよ。準備はいい？

### 4.1 Transformerがやろうとしていること：入力文から出力文への「変換」

  まず、Transformer君が、一体全体何をするための機械なのか、って話から。

  一言でいうと、Transformerは超高性能な「変換装置」なの。
  何か文章を入れると、それを別の文章にイイ感じに変換して出してくれる。

  【Transformerの仕事の例】

   * 翻訳：
       * 入力：「こんにちは、世界」
       * 出力：「Hello, world」
   * Q&A：
       * 入力：「日本の首都はどこですか？」
       * 出力：「日本の首都は東京です。」
   * 文章の要約：
       * 入力：「今日は天気が良くて、気温も暖かく、絶好のお出かけ日和なので、私は公園にピクニックに行きました。」
       * 出力：「天気が良かったので、公園にピクニックに行った。」

  みたいにね！
  入力された文章（情報のカタマリ）を、AIが理解して、別の形の文章（情報のカタマリ）に「変換（Transform）」する。だから「Transformer」って名前なんだ。ちょー分かりやすいっし
  ょ！

  この「入力→変換→出力」っていう流れのモデルを、専門用語で「Encoder-Decoder（エンコーダー・デコーダー）モデル」って言うんだ。
  第3章で出てきたSeq2Seqの考え方を、RNNなしのAttentionだけで、もっと最強にしたバージョンって思ってくれればOK！

### 4.2 6つの箱の積み重ね：EncoderとDecoderの役割分担

  じゃあ、論文のあの有名な図を思い出してみて。
  左側にデカい箱があって、右側にもデカい箱がある、アレね。

![](pic02.png)

  あれ、実は二つのチームに分かれてるんだ。

   * Encoder（エンコーダー）チーム（図の左側）：
      入力された文章を読んで、その意味を徹底的に理解するプロ集団。
   * Decoder（デコーダー）チーム（図の右側）：
      Encoderが理解した内容を元に、出力する文章を一個ずつ作り出すプロ集団。

  見事な役割分担だよね！

  で、もっとヤバいのが、論文の図をよーく見ると、EncoderもDecoderも「Nx」って書いてあって、箱が何個も積み重なってるでしょ？
  論文では、この箱をそれぞれ6個ずつ積み重ねてるんだ。

  これを例えるなら、EncoderもDecoderも、それぞれ6階建てのオフィスビルみたいな感じ！

  【Encoderビルの仕事の流れ】

   1. まず、入力された文章（「こんにちは、世界」）が、ビルの1階にやってくる。
   2. 1階の担当者が、文章をざっくり読んで、基本的な意味を理解する。
   3. その結果を、エレベーターで2階に送る。
   4. 2階の担当者は、1階の結果をもとに、もうちょい複雑な文の構造とかを理解する。
   5. これを、6階まで順番に繰り返していくんだ！

  階を上がれば上がるほど、どんどんディープで、抽象的で、文章の「魂」みたいなレベルまで、深く深く理解していくイメージ。

  6階の担当者が仕事を終える頃には、入力された文章は、AIにしか分からない、究極にイケてる「意味の塊（中間表現）」に変換されてるってワケ！

  Decoderビルも、基本は同じ。
  Encoderが作った最強の「意味の塊」を全フロアで共有しながら、1階から6階まで、みんなで知恵を出し合って、「次に出力する単語は、絶対これだよね！」って、一語一語、丁寧に文章
  を作り上げていくんだ。

  処理を6回も繰り返すなんて、無駄に見える？
  でも、この「積み重ね」こそが、Transformerが人間みたいに、言葉の裏の裏まで読める、マジの理由なんだよね！