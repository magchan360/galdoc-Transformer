## 第8章：最後の仕上げと、忘れられた英雄

  ここまで、EncoderとDecoderっていう超複雑な機械の中身を見てきたよね。
  でも、まだ話してないことが2つある。
  一つは、AIがどうやって「これだ！」っていう最後の単語を決めてるのか。
  そしてもう一つは、RNNを捨てたTransformerが、どうやって「単語の順番」を理解してるのか、っていう最大の謎だ！

### 8.1 最終出力を得るためのLinear層とSoftmax層

  Decoderの6階建てビルでの仕事が終わった後、AIの頭の中には「次に出力すべき単語」のイメージが、ぼんやりと浮かんでる。
  でも、それはまだ「赤くて、果物で、甘いヤツ…」みたいな、超あいまいな概念（ベクトルのことね！）でしかないんだ。

  この「概念」を、私たちが知ってる具体的な「単語」に変換するために、最後の仕上げを行う2人の職人がいるんだ。

   1. Linear（リニア）層：辞書引きおじさん
      まず、この職人さんが、AIが知ってる全単語が載った、超巨大な辞書を持ってくる。
      そして、AIの頭の中にある「赤くて、果物で…」っていう概念と、辞書の単語を一個ずつ見比べて、「可能性スコア」を付けていくんだ。

       * 「リンゴ」のスコア：9.8
       * 「イチゴ」のスコア：7.5
       * 「太陽」のスコア：2.1
       * 「机」のスコア：-15.3
       * ...

      みたいにね！

   2. Softmax（ソフトマックス）層：確率変換の女神さま
      次に、この女神さまが、さっきのスコアを、もっと分かりやすい「確率」に変換してくれる。

       * 「リンゴ」の確率：85%
       * 「イチゴ」の確率：13%
       * 「太陽」の確率：1%
       * その他：ほぼ0%

      って感じ！
      そして最後に、AIはこの中で一番確率が高い「リンゴ」を選んで、「これが答えだ！」って出力するんだ。

  この2ステップがあるから、AIはぼんやりしたイメージから、自信を持って一つの単語を選び出すことができるってワケ！

### 8.2 「順番」の情報を与える魔法：Positional Encodingの数学的意味

  さて、いよいよ最後の謎解き。
  Transformerは、計算を速くするために、文章を単語ごとにバラバラにして、一気に処理するって話、したよね。
  でもそれじゃ、「私が猫を好き」と「猫が私を好き」の区別がつかなくなっちゃうじゃん！

  この超ヤバい問題を解決するのが、今まで隠されてきた真の英雄、「Positional Encoding（ポディショナル・エンコーディング）」なんだ！

  これは、文章がEncoderに入力される、一番最初の段階で使われる魔法。
  一言でいうと、単語のベクトルに、「君は何番目の単語だよ」っていう位置情報を、直接足し算しちゃうの！

  【例】「猫 が 好き」という文章の場合

   1. まず、それぞれの単語を「意味」のベクトルに変換する。（Word2Vecとかでね）
       * 猫（意味のベクトル）
       * が（意味のベクトル）
       * 好き（意味のベクトル）
   2. 次に、それぞれの「位置」を表す、特殊なベクトルを用意する。
       * 1番目（位置のベクトル）
       * 2番目（位置のベクトル）
       * 3番目（位置のベクトル）
   3. 最後に、この二つを合体させる！
       * 猫（意味） + 1番目（位置） = 新生「猫」ベクトル
       * が（意味） + 2番目（位置） = 新生「が」ベクトル
       * 好き（意味）+ 3番目（位置） = 新生「好き」ベクトル

  こうすることで、AIは単語の「意味」と「位置」を、同時に理解できるようになったんだ！

  【なんでsin/cosっていう波を使うの？】

  じゃあ、この「位置のベクトル」って、どうやって作ってるの？
  ただ「1, 2, 3...」って数字を足してるわけじゃないんだ。
  論文では、sin（サイン）とcos（コサイン）っていう、周期の違うたくさんの「波」を使ってる。

  なんで波なの？ それがマジで天才的なんだ！

  理由①：相対的な位置関係が分かりやすい！
  「2番目の単語から見て、5番目の単語は、3つ離れてる」
  「10番目の単語から見て、13番目の単語も、3つ離れてる」
  この「3つ離れてる」っていう相対的な関係を、波はちょー簡単に計算できるんだ！
  固定の数字だと、こういう柔軟な関係性の表現がニガテなんだよね。

  理由②：どんなに長い文章にも対応できる！
  波って、どこまでも無限に続いていくでしょ？
  だから、文章が100単語でも、1000単語でも、AIは新しい位置情報をいくらでも作り出せる。未知の長さの文章が来ても、全然パニクらないってワケ！

  ---

  このPositional Encodingは、論文の中でもサラッとしか書かれてないけど、マジで超重要。
  これがあったからこそ、TransformerはRNNっていう「順番」の呪縛から解き放たれて、Attentionっていう翼で、自由に飛び回れるようになったんだ。

  まさに、忘れられた英雄って感じじゃない！？


---

### Q1：なんで波を使うと良いのか、ちょっと分からなかったです。  

✦ オタ君、マジでいい質問！そこ、超大事なポイントだから！
  「波を使えばOK！」って言われても、なんで？ってなるの、ちょー分かる！
  アゲハが、そこの「なんで？」の部分を、もっともっと深掘りして説明しちゃうね！

  ---

  なんで波（sin/cos）だと上手くいくの？

  これを理解するには、超たくさんの針（はり）が付いてる、特殊なアナログ時計をイメージするのが一番分かりやすい！

  この時計には、普通の「秒針」「分針」「時針」だけじゃなくて、
   * 1秒で1周する、超せっかちな針
   * 10秒で1周する、ちょっと速い針
   * 100秒で1周する、普通の針
   * 1000秒で1周する、超のんびりな針

  …みたいに、動くスピード（周波数っていうよ！）が違う針が、何十本も付いてると思って！
  この一本一本の針の「角度」が、Positional Encodingのベクトルの一つの次元に対応してるんだ。

  ---

  謎①：なんで「相対的な位置」が分かるの？

  これがマジですごいとこ！
  AIは、「N番目の単語の針の位置」と「M番目の単語の針の位置」を、直接比べる必要がないんだ。

  AIが学習するのは、「針を『Kステップ分』進めるには、どういう計算をすればいいか」っていう、回転のルールだけ！

  【例】「3つ隣の単語」の関係性を知りたい場合

  AIはこう考える。
  「今の単語の、全部の針の位置は分かってる。じゃあ、全部の針を『3ステップ分』だけ、それぞれのスピードで前に進めてみよう！」

   * せっかちな針は、ちょこっと動くだけで、角度がぐるん！と大きく変わる。
   * のんびりな針は、3ステップ進めても、ほんのちょっとしか角度が変わらない。

  この「全種類の針を、3ステップ分だけ進めた時の、針の位置のセット」っていうのが、AIにとっての「3つ隣」っていう関係性の正体なんだ！

  この「回転させる」っていう計算は、数学的にちょー簡単。
  だから、AIは「1番目と4番目の関係」も「97番目と100番目の関係」も、同じ「3ステップ回転」っていう一つのルールで理解できる。
  いちいち「1番目の場所はここで、4番目の場所はここで…」って、場所そのものを覚えなくていい。これが、相対的な位置が分かるっていうことの、本当の意味なんだ！

  ---

  謎②：なんで「長い文章」に対応できるの？

  こっちも、時計の例えで一発で分かるよ！

  もし、位置情報がただの「1, 2, 3...」っていう数字だったらどうなる？
  AIが100単語までの文章しか見たことなかったら、「101」っていう数字は、AIにとって「未知の数字」になっちゃう。「え、101って何？見たことないんだけど…」ってパニックになるんだ
  。

  でも、波（時計の針）なら、そんなことにはならない！

  なぜなら、それぞれの針が示す「角度」の組み合わせは、ほぼ無限にユニークだから！

   * 1秒後の、全種類の針の位置の組み合わせ
   * 2秒後の、全種類の針の位置の組み合わせ
   * ...
   * 100秒後の、全種類の針の位置の組み合わせ

  この組み合わせは、全部違うよね。
  じゃあ、AIが今まで見たことない「101秒後」が来たらどうなる？

  AIは全然パニクらない。
  「OK、じゃあ全部の針を、もう1ステップ分だけ進めればいいんでしょ？」
  って、今までと同じルールで、101番目の位置ベクトルを自分で計算して作り出せるんだ！

  波（sin/cos）は、数学的にどこまでもずーっと続いていく関数だから、AIは「5000番目」とか「1万番目」の位置情報だって、必要ならいつでも計算できる。

  だから、トレーニングで見たことない長さの文章が来ても、
  「未知の数字が来た！」じゃなくて、
  「既知のルールで、新しい位置を作ればいいだけじゃん！」
  って、超クールに対応できるってワケ！

  どうかな？
  波を使うっていうのは、ただの数字じゃなくて、柔軟な「ルール」と、無限に広がる「システム」をAIに与えてあげるための、マジで天才的なアイデアだったんだよね！

---
### Q2： でも、sin / cos で長くなっていったら違うものでも同じ数字になるんじゃ？  

✦ オタ君、マジで鋭すぎる！そこ、この話の核心の核心だよ！
  その疑問、天才じゃん！

  その通り！ もし、波が1種類だけだったら、オタ君の言う通り、絶対に同じ数字になっちゃう。
  例えば、sinの波が1個だけだったら、「位置0」と「位置6.28...」と「位置12.56...」は、AIから見たら全部同じ「0」に見えちゃう。これじゃ、全然区別がつかないよね。

  じゃあ、なんでTransformerは大丈夫なの？
  それは、AIが1種類の波だけじゃなくて、周期（スピード）が違う、超たくさんの波を、同時に見てるからなんだ！

  ---

#### 謎を解く鍵：周波数の違う「波のオーケストラ」

  もう一回、あの特殊なアナログ時計の例えに戻ってみよう！
  でも、今度はもっと具体的に！

  この時計には、2本の針が付いてるとする。

   1. せっかちな秒針（高周波の波）： 10秒で1周する。
   2. のんびりな分針（低周波の波）： 60秒で1周する。

  Positional Encodingは、この両方の針が、今どの角度を指してるかを、セットで記録するんだ。
  これが、単語の「位置の指紋（フィンガープリント）」になる！

  【実際に見てみよう！】

   * 位置0秒地点：
       * 秒針：12時（角度0°）
       * 分針：12時（角度0°）
       * 指紋：`[12時, 12時]`

   * 位置10秒地点：
       * せっかちな秒針は、ちょうど1周して12時に戻ってきた！
       * でも、のんびりな分針は、まだちょっとしか進んでなくて、2時の位置にいる。
       * 指紋：`[12時, 2時]`

   * 位置20秒地点：
       * 秒針は、また1周して12時に戻ってきた！
       * 分針は、さらに進んで、4時の位置にいる。
       * 指紋：`[12時, 4時]`

   * 位置60秒地点：
       * 秒針は、6周して12時にいる。
       * そして、分針が、ここでやっと1周して12時に戻ってきた！
       * 指紋：`[12時, 12時]` ← ここで初めて、位置0と同じ指紋になった！

  ---

#### 結論：なぜ衝突しないのか？

  もう分かったかな？

  たった2本の針でさえ、両方が同時に12時に戻ってくるのは、60秒に一回しか起きない。

  Transformerが使ってるPositional Encodingは、こんなもんじゃない。
  ベクトルの次元数が512だとしたら、それはスピードが違う256組（sinとcosのペアだから）の針が、同時に動いてるのと同じなんだ！

   * 超せっかちな針
   * ちょっとせっかちな針
   * 普通の針
   * のんびりな針
   * 超のんびりな針
   * ...

  これら全部の針が、全部同時に、スタート地点の12時に戻ってくるなんてこと、天文学的な時間がかからないと、まず起こらない！
  文章の長さが数千単語くらいじゃ、絶対に指紋は衝突（コリジョン）しないように、波のスピードが超巧妙に設計されてるんだ。

  だから、AIは単語の位置を、ほぼユニークな「指紋」として、正確に区別できるってワケ！
  マジで、考えた人、天才すぎでしょ！
